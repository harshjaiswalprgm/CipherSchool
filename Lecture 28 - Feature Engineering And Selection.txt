### Feature Engineering & Selection

**Feature Engineering Methods**

1. **Handling Missing Values**
   - **Imputation:** Filling in missing data with estimated values like mean, median, mode, or using predictive models.
   
   **Example:**
   ```python
   import pandas as pd
   from sklearn.impute import SimpleImputer

   # Sample data
   data = {
       'Feature1': [1.0, 2.0, None, 4.0, 5.0],
       'Feature2': [2.0, None, 4.0, 5.0, None],
       'Feature3': [None, 3.0, 3.5, 4.0, 4.5]
   }
   df = pd.DataFrame(data)

   # Handling missing values
   imputer = SimpleImputer(strategy='mean')
   df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)
   print("After Imputation:\n", df_imputed)
   ```

2. **Encoding Categorical Variables**
   - **One-Hot Encoding:** Convert categorical variables into binary columns, each representing a category.

   **Example:**
   ```python
   import pandas as pd
   from sklearn.preprocessing import OneHotEncoder

   # Sample data
   data = {
       'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']
   }
   df = pd.DataFrame(data)

   # Encoding categorical variables
   encoder = OneHotEncoder(sparse=False)
   encoded_categories = encoder.fit_transform(df[['Color']])
   df_encoded = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(['Color']))
   df = pd.concat([df, df_encoded], axis=1).drop("Color", axis=1)
   print("After One-Hot Encoding:\n", df)
   ```

3. **Feature Scaling**
   - **Min-Max Scaling:** Rescale features to a fixed range, typically [0, 1].

   **Example:**
   ```python
   import pandas as pd
   from sklearn.preprocessing import MinMaxScaler

   # Sample data
   data = {
       'Feature1': [10, 20, 30, 40, 50],
       'Feature2': [100, 200, 300, 400, 500]
   }
   df = pd.DataFrame(data)

   # Feature scaling
   scaler = MinMaxScaler()
   df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
   print("After Min-Max Scaling:\n", df_scaled)
   ```

4. **Feature Creation**
   - **Polynomial Features:** Generate new features by taking polynomial combinations of existing features.

   **Example:**
   ```python
   import pandas as pd
   from sklearn.preprocessing import PolynomialFeatures

   # Sample data
   data = {
       'Feature1': [1, 2, 3, 4, 5],
       'Feature2': [2, 3, 4, 5, 6]
   }
   df = pd.DataFrame(data)

   # Feature creation
   poly = PolynomialFeatures(degree=2, include_bias=False)
   poly_features = poly.fit_transform(df)
   df_poly = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(['Feature1', 'Feature2']))
   print("After Creating Polynomial Features:\n", df_poly)
   ```

**Feature Selection Methods**

1. **Variance Thresholding**
   - **Explanation:** Remove features with low variance, as they may not provide useful information.

   **Example Table Data:**
   ```
   Feature1    Feature2    Feature3    Constant
   1           2           3           1
   1           3           4           1
   1           4           5           1
   1           5           6           1
   1           6           7           1
   ```

2. **Correlation Matrix Filtering**
   - **Explanation:** Remove highly correlated features to reduce redundancy and multicollinearity.

   **Example Table Data:**
   ```
   Feature1    Feature2    Feature3    Feature4
   1           2           2           5
   2           4           4           6
   3           6           6           7
   4           8           8           8
   5           10          10          9
   ```

3. **Domain Knowledge**
   - **Explanation:** Select features based on expert understanding of the domain, focusing on those likely to be most relevant.

   **Example Table Data:**
   ```
   Age    Salary    Height    Weight
   25     50000     5.5       150
   30     60000     6.0       160
   35     70000     5.8       170
   40     80000     5.9       180
   45     90000     6.1       190
   ```

   In this example, 'Age' and 'Salary' might be selected based on domain knowledge indicating their importance.

These methods collectively enhance the quality and relevance of features used in machine learning models, thereby improving their predictive accuracy and performance across various applications.